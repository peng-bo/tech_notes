<attachment contenteditable="false" data-atts="%5B%5D" data-aid=".atts-b4f2d9c4-7238-4819-b809-65cec8ff1ec1"></attachment><h2>Logstash工作原理</h2><p>由于Kafka采用解耦的设计思想，并非原始的发布订阅，生产者负责产生消息，直接推送给消费者。而是在中间加入持久化层——broker,生产者把数据存放在broker中，消费者从broker中取数据。这样就带来了几个好处:</p><ul><li>1 生产者的负载与消费者的负载解耦</li><li>2 消费者按照自己的能力fetch数据</li><li>3 消费者可以自定义消费的数量</li></ul><p>另外，由于broker采用了主题topic--&gt;分区的思想，使得某个分区内部的顺序可以保证有序性，但是分区间的数据不保证有序性。这样，消费者可以以分区为单位，自定义读取的位置——offset。</p><p>Kafka采用zookeeper作为管理，记录了producer到broker的信息，以及consumer与broker中partition的对应关系。因此，生产者可以直接把数据传递给broker，broker通过zookeeper进行leader--&gt;followers的选举管理；消费者通过zookeeper保存读取的位置offset以及读取的topic的partition分区信息。</p><p><img src="https://images2015.cnblogs.com/blog/449064/201608/449064-20160804212941856-303257932.png"></p><p>由于上面的架构设计，使得生产者与broker相连；消费者与zookeeper相连。有了这样的对应关系，就容易部署logstash--&gt;kafka--&gt;logstash的方案了。</p><h4>接下来，按照下面的步骤就可以实现logstash与kafka的对接了。</h4><p><img src="https://images2015.cnblogs.com/blog/449064/201608/449064-20160804212951497-1844100718.png"></p><h2>启动kafka</h2><p>启动zookeeper：</p><pre class="ql-syntax" spellcheck="false">$zookeeper/bin/zkServer.sh start
</pre><p>启动kafka：</p><pre class="ql-syntax" spellcheck="false">$kafka/bin/kafka-server-start.sh $kafka/config/server.properties &amp;
</pre><h2>创建主题</h2><p>创建主题:</p><pre class="ql-syntax" spellcheck="false">$kafka/bin/kafka-topics.sh --zookeeper 127.0.0.1:2181 --create --topic hello --replication-factor 1 --partitions 1
</pre><p>查看主题:</p><pre class="ql-syntax" spellcheck="false">$kafka/bin/kafka-topics.sh --zookeeper 127.0.0.1:2181 --describe
</pre><h2>测试环境</h2><p>执行生产者脚本:</p><pre class="ql-syntax" spellcheck="false">$kafka/bin/kafka-console-producer.sh --broker-list 10.0.67.101:9092 --topic hello
</pre><p>执行消费者脚本，查看是否写入:</p><pre class="ql-syntax" spellcheck="false">$kafka/bin/kafka-console-consumer.sh --zookeeper 127.0.0.1:2181 --from-beginning --topic hello
</pre><h2>输入测试</h2><pre class="ql-syntax" spellcheck="false">input{
	stdin{}
}
output{
	kafka{
		topic_id =&gt; "hello"
		bootstrap_servers =&gt; "192.168.0.4:9092" # kafka的地址
		batch_size =&gt; 5
	}
	stdout{
		codec =&gt; rubydebug
	}
}
</pre><h2>读取测试</h2><p>logstash配置文件:</p><pre class="ql-syntax" spellcheck="false">input{
    kafka {
        codec =&gt; "plain"
        group_id =&gt; "logstash1"
        auto_offset_reset =&gt; "smallest"
        reset_beginning =&gt; true
        topic_id =&gt; "hello"
        #white_list =&gt; ["hello"]
        #black_list =&gt; nil
        zk_connect =&gt; "192.168.0.5:2181" # zookeeper的地址
   }

}
output{
    stdout{
        codec =&gt; rubydebug
    }
}
</pre><p><br></p>